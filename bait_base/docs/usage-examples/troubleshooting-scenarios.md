# Troubleshooting Scenarios with bAIt

This document provides real-world troubleshooting scenarios showing how bAIt helps diagnose and resolve common beamline issues.

## Scenario 1: Detector Not Responding

### Initial Problem Report
**User**: "The detector screen is black, and I can't collect any data."

### bAIt Diagnostic Process

#### Step 1: Initial Assessment
```bash
$ bait-query 8id-bits "The detector is not responding, help me diagnose the issue"

Detector Diagnostic Analysis:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ” Detector Status Check:
â€¢ Detector IOC (detector_ioc_1): âŒ Not responding
â€¢ Last communication: 23 minutes ago
â€¢ IOC process status: âŒ Not running
â€¢ Power supply status: âœ… Normal (24V)

ğŸ”— System Dependencies:
â€¢ MEDM screens affected: 3 (detector_main.adl, detector_expert.adl, detector_config.adl)
â€¢ Bluesky devices affected: 2 (lambda_detector, pilatus_detector)
â€¢ Current experiments: 1 scan paused, 3 scans queued

ğŸš¨ Immediate Actions Required:
1. Restart detector IOC on host 8id-ioc2
2. Verify detector hardware connections
3. Check network connectivity
4. Resume paused experiments once resolved
```

#### Step 2: Detailed Analysis
```bash
$ bait-analyze 8id-bits --focus detector --verbose

Detailed Detector Analysis:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ–¥ï¸  IOC Host Status (8id-ioc2):
â€¢ Host responsive: âœ… Yes
â€¢ CPU usage: 15% (normal)
â€¢ Memory: 2.3GB used / 8GB total
â€¢ Disk space: 78% used (normal)

ğŸ”Œ Hardware Connections:
â€¢ Detector power: âœ… 24V stable
â€¢ Network cable: âœ… Link detected
â€¢ Cooling system: âœ… 42Â°C (normal)
â€¢ Trigger cable: âš ï¸ May be loose

ğŸ“ Error Log Analysis:
â€¢ Last error: "Communication timeout at 14:23:45"
â€¢ Frequency: 3 timeouts in last hour
â€¢ Pattern: Intermittent connection issues

ğŸ”§ Root Cause Analysis:
Most likely cause: Loose network connection
Secondary cause: IOC software hang
Evidence: Intermittent timeouts, hardware OK
```

#### Step 3: Resolution Steps
```bash
$ bait-query 8id-bits "Provide step-by-step resolution for detector communication timeout"

Resolution Steps: Detector Communication Timeout
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”§ Immediate Actions:
1. Check network cable at detector:
   â€¢ Visually inspect for damage
   â€¢ Ensure secure connection
   â€¢ Check LED indicators (should be green)

2. Restart detector IOC:
   â€¢ SSH to 8id-ioc2: ssh controls@8id-ioc2
   â€¢ Check process: ps aux | grep detector
   â€¢ Kill if running: pkill detector_ioc
   â€¢ Restart: /home/controls/iocs/detector_ioc/st.cmd

3. Verify communication:
   â€¢ Test PV connection: caget DET1:CAM:Status
   â€¢ Check MEDM screen: detector_main.adl
   â€¢ Verify detector response: detector self-test

â±ï¸  Expected Resolution Time: 5-10 minutes

ğŸ”„ Verification Steps:
â€¢ IOC status: Should show "Running"
â€¢ PV values: Should update normally
â€¢ MEDM screen: Should show live data
â€¢ Test acquisition: Take test image

ğŸ“Š Success Criteria:
â€¢ All detector PVs responding
â€¢ MEDM screens updating
â€¢ Test image acquired successfully
â€¢ No timeout errors in logs
```

### Resolution Outcome
```bash
$ bait-query 8id-bits "Verify detector issue resolution"

Resolution Verification:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Issue Resolved Successfully:
â€¢ Detector IOC restarted and running
â€¢ Network connection stabilized
â€¢ All PVs responding normally
â€¢ MEDM screens operational

ğŸ“Š Post-Resolution Status:
â€¢ Detector temperature: 42Â°C (normal)
â€¢ Communication latency: 15ms (excellent)
â€¢ Error rate: 0% (last 30 minutes)
â€¢ Experiments resumed: 4 scans completed

ğŸ”§ Actions Taken:
â€¢ Tightened network cable connection
â€¢ Restarted detector IOC
â€¢ Verified all dependent systems
â€¢ Updated maintenance log

ğŸ“ˆ Prevention Measures:
â€¢ Schedule cable inspection monthly
â€¢ Add network monitoring alerts
â€¢ Update IOC watchdog settings
â€¢ Train operators on quick diagnostics
```

## Scenario 2: Motor Moving Slowly

### Initial Problem Report
**User**: "The sample positioning is taking forever. The motor is moving much slower than usual."

### bAIt Diagnostic Process

#### Step 1: Motor Performance Analysis
```bash
$ bait-query 8id-bits "The sample motor is moving slowly, analyze performance"

Motor Performance Analysis:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ” Sample X Motor (8idimot1:m1):
â€¢ Current velocity: 0.3 mm/s
â€¢ Configured velocity: 2.0 mm/s
â€¢ Actual performance: 15% of expected
â€¢ Motor controller: ESP301 #1

âš¡ Power and Control:
â€¢ Supply voltage: 22.1V (normal: 24V)
â€¢ Current draw: 0.8A (normal: 0.6A)
â€¢ Temperature: 71Â°C (normal: <60Â°C)
â€¢ Controller status: Active warnings

ğŸ”§ Potential Issues Identified:
1. Low power supply voltage (9% below normal)
2. High operating temperature (18% above normal)
3. Increased current draw (33% above normal)
4. Controller reporting internal warnings

ğŸš¨ Immediate Concerns:
â€¢ Risk of motor damage if overheating continues
â€¢ Experiment timeline will be significantly delayed
â€¢ Other motors may be affected by power supply issue
```

#### Step 2: Root Cause Investigation
```bash
$ bait-analyze 8id-bits --focus motor_system --root-cause-analysis

Root Cause Analysis: Motor Performance
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ” Power Supply Analysis:
â€¢ Main supply: 22.1V (should be 24V Â±0.5V)
â€¢ Supply current: 4.2A (normal: 3.5A)
â€¢ Ripple: 0.3V (normal: <0.1V)
â€¢ Temperature: 68Â°C (normal: <50Â°C)

ğŸ”— Dependency Chain:
Power Supply â†’ Motor Controller â†’ Motor â†’ Positioning System

ğŸ”§ Fault Tree Analysis:
Primary Cause: Power supply degradation
â”œâ”€â”€ Aging capacitors (likely)
â”œâ”€â”€ Ventilation blockage (check)
â”œâ”€â”€ Overload condition (check)
â””â”€â”€ Component failure (possible)

ğŸ“Š Historical Data:
â€¢ Voltage trend: Declining over past 2 weeks
â€¢ Temperature trend: Rising over past month
â€¢ Performance degradation: 15% over past week

ğŸ¯ Recommended Actions:
1. IMMEDIATE: Reduce motor speeds to prevent damage
2. SHORT-TERM: Replace power supply
3. LONG-TERM: Implement power monitoring
4. PREVENTIVE: Schedule regular power supply maintenance
```

#### Step 3: Mitigation Strategy
```bash
$ bait-query 8id-bits "Provide immediate mitigation for motor power supply issues"

Mitigation Strategy: Motor Power Supply Issues
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸš¨ Immediate Actions (Next 30 minutes):
1. Reduce motor velocities to 50% of normal:
   â€¢ Access motor_control_expert.adl
   â€¢ Update velocity settings for all motors
   â€¢ Test with short moves first

2. Improve ventilation:
   â€¢ Check power supply air filters
   â€¢ Ensure cooling fans operating
   â€¢ Clear any obstructions

3. Monitor temperatures:
   â€¢ Set up continuous monitoring
   â€¢ Alert threshold: 75Â°C
   â€¢ Automatic shutdown at 80Â°C

â±ï¸  Short-term Actions (Next 24 hours):
1. Order replacement power supply:
   â€¢ Model: Acopian 24V/5A regulated
   â€¢ Delivery: Next day if ordered by 2 PM
   â€¢ Cost: ~$200

2. Prepare for replacement:
   â€¢ Schedule maintenance window
   â€¢ Notify users of brief downtime
   â€¢ Prepare backup procedures

ğŸ“Š Experiment Impact:
â€¢ Current scans: 2.5x longer than normal
â€¢ Recommended: Pause complex positioning
â€¢ Alternative: Use manual positioning for critical samples
â€¢ Timeline: Full resolution in 24-48 hours
```

### Resolution and Follow-up
```bash
$ bait-report 8id-bits --incident-report motor_power_supply --resolution-summary

Incident Report: Motor Power Supply Issue
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ Incident Summary:
â€¢ Date: 2024-01-15
â€¢ Issue: Motor performance degradation due to power supply failure
â€¢ Impact: 85% reduction in positioning speed
â€¢ Resolution time: 18 hours
â€¢ Experiments affected: 3 (temporarily paused)

ğŸ”§ Actions Taken:
â€¢ Immediate: Reduced motor speeds to prevent damage
â€¢ Short-term: Replaced power supply unit
â€¢ Long-term: Installed power monitoring system
â€¢ Preventive: Updated maintenance schedule

ğŸ“Š Results:
â€¢ Motor performance: Restored to 100%
â€¢ Power supply voltage: Stable at 24.0V Â±0.1V
â€¢ Temperature: Reduced to 45Â°C
â€¢ No equipment damage occurred

ğŸ’¡ Lessons Learned:
â€¢ Early warning system needed for power supplies
â€¢ Regular voltage monitoring prevents failures
â€¢ Spare power supplies should be kept on-site
â€¢ Staff training on power supply diagnostics improved
```

## Scenario 3: Scan Plan Execution Failures

### Initial Problem Report
**User**: "My scan keeps failing after the first few points. The queue server shows errors."

### bAIt Diagnostic Process

#### Step 1: Scan Analysis
```bash
$ bait-query 8id-bits "My scan plan keeps failing, help me debug this"

Scan Plan Debug Analysis:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ” Recent Scan Failures:
â€¢ Failed scans: 7 out of last 10 attempts
â€¢ Failure pattern: Typically fails at point 3-5
â€¢ Error type: Device timeout
â€¢ Affected plan: grid_scan_temperature

ğŸ“Š Error Pattern Analysis:
â€¢ Failure point: During detector readout
â€¢ Timeout: 30 seconds (default)
â€¢ Success rate: 30% (normally 98%+)
â€¢ Failure timing: Random, not systematic

ğŸ”— Involved Components:
â€¢ Primary: Lambda detector (detector_ioc_1)
â€¢ Secondary: Sample temperature controller (temp_ioc)
â€¢ Tertiary: Sample positioning motors (motor_ioc)

ğŸš¨ Potential Issues:
1. Detector readout timeout (most likely)
2. Temperature controller instability
3. Network communication issues
4. Insufficient wait times in plan
```

#### Step 2: Component Analysis
```bash
$ bait-analyze 8id-bits --focus scan_execution --error-analysis

Scan Execution Error Analysis:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ” Detector Performance:
â€¢ Readout time: 8.5 seconds (normal: 3.2 seconds)
â€¢ Timeout setting: 30 seconds
â€¢ Success rate: 65% (should be >95%)
â€¢ Error type: "Acquisition timeout"

ğŸŒ¡ï¸  Temperature Controller:
â€¢ Stability: Â±0.5Â°C (normal: Â±0.1Â°C)
â€¢ Response time: 45 seconds (normal: 15 seconds)
â€¢ Overshoot: 2.3Â°C (normal: <0.5Â°C)
â€¢ PID tuning: Appears suboptimal

ğŸ”§ Scan Plan Analysis:
â€¢ Wait time after temp change: 30 seconds
â€¢ Required stabilization time: 60 seconds (estimated)
â€¢ Detector exposure: 5 seconds
â€¢ Total point time: 43.5 seconds (should be ~25 seconds)

ğŸ’¡ Root Cause Identified:
Temperature controller instability causing:
1. Longer stabilization times needed
2. Detector performance degradation at unstable temperatures
3. Scan plan timing assumptions invalid
```

#### Step 3: Solution Implementation
```bash
$ bait-query 8id-bits "Provide solution for scan plan failures due to temperature instability"

Solution: Scan Plan Temperature Issues
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”§ Immediate Fixes:
1. Adjust scan plan timing:
   â€¢ Increase wait time: 30s â†’ 90s
   â€¢ Add temperature stability check
   â€¢ Implement adaptive waiting

2. Optimize temperature controller:
   â€¢ Retune PID parameters
   â€¢ Reduce overshoot settings
   â€¢ Implement gradual temperature changes

3. Modify detector settings:
   â€¢ Increase timeout: 30s â†’ 60s
   â€¢ Pre-warm detector before scan
   â€¢ Add temperature compensation

ğŸ“ Updated Scan Plan:
```python
# Original problematic plan
def grid_scan_temperature(temps):
    for temp in temps:
        yield from bps.mv(temp_controller, temp)
        yield from bps.sleep(30)  # Too short!
        yield from bp.count([detector], num=1)

# Improved plan
def grid_scan_temperature_stable(temps):
    for temp in temps:
        yield from bps.mv(temp_controller, temp)
        yield from bps.sleep(60)  # Longer wait
        # Wait for stability
        yield from wait_for_temperature_stability(temp, tolerance=0.1)
        yield from bp.count([detector], num=1)
```

ğŸ¯ Expected Results:
â€¢ Scan success rate: >95%
â€¢ Point completion time: 85 seconds
â€¢ Temperature stability: Â±0.1Â°C
â€¢ Detector timeouts: Eliminated
```

### Verification and Optimization
```bash
$ bait-query 8id-bits "Verify scan plan fixes and suggest optimizations"

Scan Plan Verification Results:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Test Results:
â€¢ Test scans completed: 15/15 (100% success)
â€¢ Average point time: 78 seconds (improved from 43.5s)
â€¢ Temperature stability: Â±0.08Â°C (excellent)
â€¢ Detector timeouts: 0 (eliminated)

ğŸ“Š Performance Metrics:
â€¢ Scan completion rate: 100%
â€¢ Data quality: Excellent (no temperature artifacts)
â€¢ Total scan time: 32 minutes (vs. 18 minutes original)
â€¢ Reliability: Significantly improved

ğŸš€ Further Optimizations:
1. Implement predictive temperature control:
   â€¢ Pre-calculate temperature trajectory
   â€¢ Start temperature changes earlier
   â€¢ Reduce total wait times

2. Add intelligent waiting:
   â€¢ Monitor temperature derivative
   â€¢ Adaptive wait times based on stability
   â€¢ Parallel detector preparation

3. Optimize PID parameters:
   â€¢ Run auto-tuning procedure
   â€¢ Optimize for scan application
   â€¢ Reduce overshoot to <0.2Â°C

ğŸ’¡ Expected Improvements:
â€¢ Scan time reduction: 15-20%
â€¢ Further reliability improvement
â€¢ Better data quality consistency
â€¢ Reduced operator intervention
```

## Scenario 4: Network Connectivity Issues

### Initial Problem Report
**User**: "MEDM screens are updating very slowly, and some PVs show disconnected."

### bAIt Diagnostic Process

#### Step 1: Network Analysis
```bash
$ bait-query 8id-bits "MEDM screens are slow and PVs are disconnected"

Network Connectivity Analysis:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸŒ Network Status:
â€¢ Subnet: 164.54.xxx.xxx/24
â€¢ Active hosts: 15/18 (3 not responding)
â€¢ Average latency: 45ms (normal: <5ms)
â€¢ Packet loss: 3.2% (normal: <0.1%)

ğŸ”— EPICS Network Status:
â€¢ CA Gateway: âš ï¸ Intermittent timeouts
â€¢ PV Access: âš ï¸ High latency
â€¢ Channel connections: 234/267 (87% connected)
â€¢ Disconnected PVs: 33 (mostly on IOC host 8id-ioc2)

ğŸ“Š Problem Areas:
â€¢ IOC host 8id-ioc2: High latency (95ms)
â€¢ Switch connection: Intermittent errors
â€¢ Network utilization: 78% (high)
â€¢ DNS resolution: Slow (2.3s average)

ğŸš¨ Impact Assessment:
â€¢ MEDM screen updates: 5-10 second delays
â€¢ Bluesky device communication: Intermittent failures
â€¢ Data collection: 15% slower than normal
â€¢ User experience: Significantly degraded
```

#### Step 2: Infrastructure Investigation
```bash
$ bait-analyze 8id-bits --network-diagnostics --infrastructure-check

Network Infrastructure Analysis:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ” Physical Layer:
â€¢ Switch status: 2 ports showing errors
â€¢ Cable integrity: 1 cable with high error rate
â€¢ Port utilization: 3 ports >90% usage
â€¢ Power supply: All switches normal

ğŸ“ˆ Traffic Analysis:
â€¢ Broadcast storms: None detected
â€¢ Bandwidth usage: 78% on main trunk
â€¢ Protocol distribution: 45% EPICS, 30% SSH, 25% other
â€¢ Error rate: 0.3% (elevated)

ğŸ–¥ï¸  Host Performance:
â€¢ 8id-ioc2: CPU 85%, Memory 92% (overloaded)
â€¢ 8id-ws1: Normal performance
â€¢ 8id-gateway: Normal performance
â€¢ Network stack: TCP retransmissions elevated

ğŸ”§ Root Cause Analysis:
Primary: IOC host 8id-ioc2 overloaded
Secondary: Network cable degradation
Tertiary: High network utilization

âš¡ Immediate Actions Needed:
1. Reduce load on 8id-ioc2
2. Replace degraded network cable
3. Implement traffic prioritization
4. Add network monitoring
```

#### Step 3: Resolution Strategy
```bash
$ bait-query 8id-bits "Provide network issue resolution strategy"

Network Issue Resolution Strategy:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸš¨ Immediate Actions (Next 2 hours):
1. Redistribute IOC load:
   â€¢ Move 2 IOCs from 8id-ioc2 to 8id-ioc3
   â€¢ Restart overloaded IOCs
   â€¢ Verify PV connections restored

2. Replace network cable:
   â€¢ Identify degraded cable (port 12)
   â€¢ Replace with Cat6 cable
   â€¢ Test connection integrity

3. Network optimization:
   â€¢ Implement EPICS traffic prioritization
   â€¢ Reduce broadcast frequency
   â€¢ Optimize CA Gateway settings

ğŸ“Š Expected Results:
â€¢ Latency reduction: 45ms â†’ <10ms
â€¢ PV connection rate: 87% â†’ >98%
â€¢ MEDM responsiveness: Normal
â€¢ Network utilization: 78% â†’ <60%

ğŸ”§ Long-term Improvements:
1. Add network monitoring:
   â€¢ Real-time latency monitoring
   â€¢ Automated alerts for high usage
   â€¢ Traffic analysis dashboard

2. Infrastructure upgrades:
   â€¢ Dedicated EPICS network segment
   â€¢ Redundant switch configuration
   â€¢ Improved cable management

3. Load balancing:
   â€¢ Distribute IOCs across multiple hosts
   â€¢ Implement failover mechanisms
   â€¢ Regular load assessment

â±ï¸  Timeline:
â€¢ Immediate fixes: 2 hours
â€¢ Full resolution: 24 hours
â€¢ Long-term improvements: 2 weeks
```

### Resolution Verification
```bash
$ bait-report 8id-bits --network-resolution-report

Network Resolution Report:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Resolution Summary:
â€¢ Issue: Network connectivity and performance problems
â€¢ Duration: 6 hours (detection to full resolution)
â€¢ Impact: Temporary degradation of MEDM and PV access
â€¢ Resolution: Successful, all systems normal

ğŸ“Š Post-Resolution Metrics:
â€¢ Network latency: 3.2ms (excellent)
â€¢ PV connection rate: 99.2% (excellent)
â€¢ MEDM responsiveness: Normal
â€¢ Network utilization: 52% (normal)

ğŸ”§ Actions Completed:
â€¢ Redistributed IOC load across hosts
â€¢ Replaced degraded network cable
â€¢ Implemented EPICS traffic prioritization
â€¢ Added network monitoring system

ğŸ“ˆ Improvements Achieved:
â€¢ 92% reduction in network latency
â€¢ 14% improvement in PV connection rate
â€¢ 100% improvement in MEDM responsiveness
â€¢ 33% reduction in network utilization

ğŸ’¡ Preventive Measures:
â€¢ Monthly network health checks
â€¢ Automated monitoring alerts
â€¢ Regular cable inspection
â€¢ Load balancing review quarterly
```

## Common Troubleshooting Patterns

### Pattern 1: Intermittent Issues
```bash
$ bait-query 8id-bits "Help me troubleshoot intermittent system issues"

Intermittent Issue Troubleshooting:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ” Diagnostic Approach:
1. Pattern Recognition:
   â€¢ Time-based patterns (hourly, daily, weekly)
   â€¢ Load-based patterns (high usage periods)
   â€¢ Environment-based patterns (temperature, humidity)

2. Data Collection:
   â€¢ Enable detailed logging
   â€¢ Monitor system metrics
   â€¢ Track error frequencies

3. Correlation Analysis:
   â€¢ Match errors with system events
   â€¢ Identify common factors
   â€¢ Establish cause-effect relationships

ğŸ”§ Common Causes:
â€¢ Thermal cycling effects
â€¢ Network congestion periods
â€¢ Memory leaks in long-running processes
â€¢ Timing-sensitive race conditions
â€¢ Environmental factors
```

### Pattern 2: Performance Degradation
```bash
$ bait-query 8id-bits "System performance is gradually getting worse"

Performance Degradation Analysis:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š Trending Analysis:
â€¢ CPU usage: Monitor for increasing baseline
â€¢ Memory usage: Check for memory leaks
â€¢ Network performance: Track latency trends
â€¢ Disk I/O: Monitor for fragmentation

ğŸ” Root Cause Categories:
1. Resource exhaustion (most common)
2. Software aging effects
3. Hardware degradation
4. Configuration drift
5. Environmental changes

ğŸ› ï¸  Diagnostic Tools:
â€¢ Historical performance data
â€¢ Resource utilization trends
â€¢ Error rate analysis
â€¢ Comparative benchmarking
```

This comprehensive troubleshooting guide demonstrates how bAIt provides systematic, intelligent assistance for resolving complex beamline issues efficiently and effectively.